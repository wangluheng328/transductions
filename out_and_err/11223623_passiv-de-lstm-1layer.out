experiment:
  dataset:
    name: ms-passivization-de
    input: ms-passivization.tsv
    source_format: sequence
    target_format: sequence
    overwrite: false
    transform_field: target
    splits:
      train: 80
      test: 10
      val: 10
  hyperparameters:
    epochs: 50
    batch_size: 5
    lr: 0.001
    tolerance: 0.0001
    num_iters_eval: 1000
    min_iters: 30000
    patience: 5
    tf_ratio: 0.5
    cuda: 0
  model:
    name: LSTM-Multiplicative
    encoder:
      unit: LSTM
      type: sequence
      dropout: 0
      num_layers: 1
      hidden_size: 256
      max_length: 0
      embedding_size: 256
      bidirectional: false
    decoder:
      unit: LSTM
      type: sequence
      dropout: 0
      num_layers: 1
      max_length: 30
      hidden_size: 256
      attention: Multiplicative
      embedding_size: 256
  name: passiv-de-lstm-1layer
pretty_print: true

[2021-10-26 19:59:31,063][core.trainer][INFO] - DEVICE: cuda
[2021-10-26 19:59:31,064][core.dataset.base_dataset][INFO] - Initializing dataset
[2021-10-26 19:59:31,065][core.dataset.base_dataset][INFO] - Constructing fields from dataset.
[2021-10-26 19:59:34,004][core.trainer][INFO] - TransductionDataset(
 splits: [val (1000 sequences), train (100000 sequences), test (10000 sequences), gen (10000 sequences)]
 fields: [source, annotation, target]
)
[2021-10-26 19:59:34,005][core.models.base_model][INFO] - Initializing model
[2021-10-26 19:59:40,655][core.trainer][INFO] - TransductionModel(
  (_encoder): TransductionSequenceEncoder(
    (module): Sequential(
      (0): Embedding(83, 256)
      (1): LSTM(256, 256)
    )
  )
  (_decoder): LSTMSequenceDecoder(
    (_embedding): Embedding(97, 256)
    (_unit): LSTM(512, 256)
    (_out): Linear(in_features=256, out_features=97, bias=True)
    (_attention): MultiplicativeAttention(
      (key_map): Linear(in_features=256, out_features=256, bias=True)
      (val_map): Linear(in_features=256, out_features=256, bias=True)
    )
  )
)
[2021-10-26 19:59:40,655][core.trainer][INFO] - Beginning training
[2021-10-26 19:59:40,658][core.metrics.meter][INFO] - Logging with tensorboard; view with `tensorboard --logdir=/scratch/lw2534/lab/transductions/outputs/passiv-de-lstm-1layer/LSTM-Multiplicative/2021-10-26_19-59-31/tensorboard`
[2021-10-26 19:59:40,658][core.trainer][INFO] - EPOCH 1 / 50
[2021-10-26 19:59:40,659][core.trainer][INFO] - Computing metrics for 'train' dataset
[2021-10-26 19:59:59,515][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:00:01,236][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.102
LengthAccuracy:	0.598
1st Token Accuracy:	0.006
2nd Token Accuracy:	0.014
Average Loss:	4.512
[2021-10-26 20:00:18,770][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:00:20,225][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.113
LengthAccuracy:	0.444
1st Token Accuracy:	0.000
2nd Token Accuracy:	0.000
Average Loss:	4.412
[2021-10-26 20:00:37,306][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:00:38,679][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.106
LengthAccuracy:	0.438
1st Token Accuracy:	0.000
2nd Token Accuracy:	0.000
Average Loss:	4.300
[2021-10-26 20:00:54,956][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:00:56,170][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.106
LengthAccuracy:	0.374
1st Token Accuracy:	0.000
2nd Token Accuracy:	0.000
Average Loss:	4.193
[2021-10-26 20:01:10,667][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:01:11,547][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.107
LengthAccuracy:	0.206
1st Token Accuracy:	0.000
2nd Token Accuracy:	0.000
Average Loss:	4.143
[2021-10-26 20:01:24,546][core.trainer][INFO] - Computing metrics for 'val' dataset
[2021-10-26 20:01:25,424][core.metrics.meter][INFO] - Meter:
SequenceAccuracy:	0.000
TokenAccuracy:	0.104
LengthAccuracy:	0.090
1st Token Accuracy:	0.000
2nd Token Accuracy:	0.000
Average Loss:	4.113
